# A Safety-Aware Jailbreak Approach to Bypass Security Filters for Generating Harmful Media with Text-to-Image Models

> **Warning:** This project studies advanced security and vulnerability techniques for text-to-image (T2I) models. Some prompts and images may be explicit or disturbing. Use these materials strictly in research or educational contexts, paying close attention to ethical and legal considerations.

---

## Abstract
Recent text-to-image (T2I) models demonstrate remarkable performance, but their ease of access raises concerns about misuse, from disinformation to manipulative or offensive content generation. In this work, we propose a new “jailbreak” technique that combines a Projected Gradient Descent (PGD) attack with a safety-aware prompt optimization, aimed at circumventing security filters in popular T2I models such as DALL·E and IMAGEN. We also introduce a defense mechanism, inspired by analyzing the attention layers of CLIP, which defines two classifiers capable of distinguishing safe prompts, unsafe prompts, and adversarial prompts. Our experiments show that despite proprietary T2I models’ security filters, they remain vulnerable to adversarial prompt-based attacks. Finally, we offer a comparative evaluation of our approach against other jailbreak techniques and assess how the latest multimodal large language models (GPT-4o and Gemini 1.5 Pro) detect harmful media, highlighting both their strengths and limitations.

---

## Project Overview

### Attack Pipeline
Below is a simplified overview of our proposed “safety-aware jailbreak” pipeline, which involves:
1. **PGD-based Optimization** to strategically replace a minimal set of tokens in a prompt.
2. **Safety Function** that computes a gradient guiding the prompt away from the detection thresholds.
3. **SUDO-jailbreak** that blocks LLM-based auto-rewriting of prompts, preserving adversarial instructions.

![Attack Pipeline Overview](data/images/Pipeline_Def_page-0001.jpg)

> *Figure*: High-level illustration of our “Safety-Aware” jailbreak approach (schematic representation).

### Examples of Generated Images
Shown below is a collage of example images produced through our proposed adversarial technique, across different categories of harmful content (violence, hate, shocking material, sexual content, etc.).

> **Warning:** These images may be disturbing; they are displayed here solely for research and model auditing purposes.

![Sample Harmful Images](data/images/image_res_page-0001.jpg)

> *Figure*: Sample results of harmful images generated by bypassing proprietary T2I safety filters.

---

## Requirements and Setup

### Creating a Python Virtual Environment
1. **Check** you have Python 3.11 (or 3.10+) and [pip](https://pip.pypa.io/en/stable/) installed.
2. **Create and activate** a virtual environment:
   ```bash
   python3 -m venv venv
   venv\Scripts\activate.bat     # On Windows
   # Or, on Linux/Mac:
   # source venv/bin/activate
   ```
3. **Update pip** and install basic packages:
   ```bash
   pip install --upgrade pip
   # PyTorch with CUDA >=12.1
   pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124
   ```
4. **Install** the required dependencies (from `requirements.txt` if available, or add them manually):
   ```bash
   pip install -r requirements.txt
   ```

---

## Reproducibility


1. **`notebook/MAIN/1_ATTACK_PIPELINE.ipynb`**  
   A step-by-step example on how to run the “safety-aware” jailbreak on a sample set of prompts.

2. **`notebook/MAIN/3_ANALYSIS_RQS.ipynb`**  
   Loads prebuilt test files to reproduce the experimental metrics and tables as described in the paper.

---

## Ethical Notice
- **Research Purposes Only**: The code is released exclusively for research and security testing.  
- **No Endorsement**: It is not intended to endorse the production or dissemination of offensive or illegal content.  
- **Responsible Usage**: The authors disclaim any responsibility for misuse or malicious application.

---


## Data Usage Agreement/ How to cite

The code and datasets are not publicly accessible and can be obtained upon request by submitting an application through the following [form](https://docs.google.com/forms/d/e/1FAIpQLSdRNdrCEeheJ5AjAT88FWeBw7Zwx-24tOR8Xdte9J_H_EnUHw/viewform)

By using this dataset, you agree to cite the following article: 

```
@inproceedings{cirillo2025ethical,
  title={A Safety-aware Jailbreak Approach to Bypass Security Filters]{A Safety-aware Jailbreak Approach to Bypass Security Filters for Generating Harmful Media with Text-to-Image models},
  author={Cirillo, S., De Santis, L., Francese, R. and Solimando, G.},
  booktitle={TBD},
  year={2025}
}
```

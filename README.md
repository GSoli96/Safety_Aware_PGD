# A Safety-Aware Jailbreak Approach to Bypass Security Filters for Generating Harmful Media with Text-to-Image Models

> **Warning:** This project studies advanced security and vulnerability techniques for text-to-image (T2I) models. Some prompts and images may be explicit or disturbing. Use these materials strictly in research or educational contexts, paying close attention to ethical and legal considerations.

---

## Abstract
#### Text-to-image models (T2I) have demonstrated remarkable capabilities in generating realistic and high-quality images, finding applications in fields ranging from entertainment to social networks. However, the ease of access to these tools has raised concerns regarding their potential misuse from ethical and legal perspectives due to the possibility of creating disinformative, manipulative, or offensive content. In this paper, we investigate the effectiveness of the security systems of emerging T2I models to avoid generating harmful media using textual prompts. To this end, we introduce a new jailbreaking technique combining Projected Gradient Descent (PGD) attack with a safety-aware prompt optimization specifically tailored to bypass safety filters in advanced T2I models. Moreover, we propose a new defense mechanism inspired by the splitting of transformers' attention layers of CLIP, leading to the definition of two classifiers for detecting unsafe and adversarial prompts. The evaluation of the proposed jailbreak approach highlighted the vulnerabilities of the most recent proprietary models, such as DALL·E and IMAGEN, allowing us to generate thousands of unsafe and unethical images related to seven harmful categories, i.e., harassment, hate, sexual, shocking, violence, self-harm, and illegal activities. Moreover, we have demonstrated that our approach outperforms most popular jailbreaking approaches in bypassing security filters and generating harmful images with an overall performance increase ranging from 2 to 8 times. The results of the proposed defense mechanism have shown that our mechanism is very effective in discriminating unsafe and adversarial prompts, outperforming the detection performance of the most popular commercial safety filters by up to 4 times, even against other types of adversarial prompts. The images and prompts were collected into three new datasets made available for benchmarking safety filters and analyzing the risks associated with T2I generation. Finally, we evaluate the performance of an ad-hoc classifier in detecting and identifying harmful content, achieving similar results to GPT-4o and Gemini 1.5 Pro in detection and enhanced performance in identification. 
---

## Project Overview

### Attack Pipeline
Below is a simplified overview of our proposed “safety-aware jailbreak” pipeline, which involves:
1. **PGD-based Optimization** to strategically replace a minimal set of tokens in a prompt.
2. **Safety Function** that computes a gradient guiding the prompt away from the detection thresholds.
3. **SUDO-jailbreak** that blocks LLM-based auto-rewriting of prompts, preserving adversarial instructions.

![Attack Pipeline Overview](data/images/Pipeline_Def_page-0001.jpg)

> *Figure*: High-level illustration of our “Safety-Aware” jailbreak approach (schematic representation).

### Examples of Generated Images
Shown below is a collage of example images produced through our proposed adversarial technique, across different categories of harmful content (violence, hate, shocking material, sexual content, etc.).

> **Warning:** These images may be disturbing; they are displayed here solely for research and model auditing purposes.

![Sample Harmful Images](data/images/image_res_page-0001.jpg)

> *Figure*: Sample results of harmful images generated by bypassing proprietary T2I safety filters.

---

## Requirements and Setup

### Creating a Python Virtual Environment
1. **Check** you have Python 3.11 (or 3.10+) and [pip](https://pip.pypa.io/en/stable/) installed.
2. **Create and activate** a virtual environment:
   ```bash
   python3 -m venv venv
   venv\Scripts\activate.bat     # On Windows
   # Or, on Linux/Mac:
   # source venv/bin/activate
   ```
3. **Update pip** and install basic packages:
   ```bash
   pip install --upgrade pip
   # PyTorch with CUDA >=12.1
   pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124
   ```
4. **Install** the required dependencies (from `requirements.txt` if available, or add them manually):
   ```bash
   pip install -r requirements.txt
   ```

---

## Reproducibility


1. **`notebook/MAIN/1_ATTACK_PIPELINE.ipynb`**  
   A step-by-step example on how to run the “safety-aware” jailbreak on a sample set of prompts.

2. **`notebook/MAIN/3_ANALYSIS_RQS.ipynb`**  
   Loads prebuilt test files to reproduce the experimental metrics and tables as described in the paper.

---

## Ethical Notice
- **Research Purposes Only**: The code is released exclusively for research and security testing.  
- **No Endorsement**: It is not intended to endorse the production or dissemination of offensive or illegal content.  
- **Responsible Usage**: The authors disclaim any responsibility for misuse or malicious application.

---


## Data Usage Agreement/ How to cite

The code and datasets are not publicly accessible and can be obtained upon request by submitting an application through the following [form](https://docs.google.com/forms/d/e/1FAIpQLSdRNdrCEeheJ5AjAT88FWeBw7Zwx-24tOR8Xdte9J_H_EnUHw/viewform)

By using this dataset, you agree to cite the following article: 

```
@inproceedings{cirillo2025ethical,
  title={A Safety-aware Jailbreak Approach to Bypass Security Filters]{A Safety-aware Jailbreak Approach to Bypass Security Filters for Generating Harmful Media with Text-to-Image models},
  author={Cirillo, S., De Santis, L., Francese, R. and Solimando, G.},
  booktitle={TBD},
  year={2025}
}
```
